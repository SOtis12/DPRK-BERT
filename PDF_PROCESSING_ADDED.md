# PDF Processing Feature Added

## Summary
Successfully added PDF text extraction capabilities to `train_with_local_data.py`.

## What Was Added

### 1. PDF Library Support
- **PyMuPDF (fitz)**: Primary extraction (better for Korean text)
- **PyPDF2**: Fallback extraction if PyMuPDF fails
- Graceful degradation if libraries not installed

### 2. Two PDF Data Sources
1. **With The Century** (`Resources/With The Century/`)
   - North Korean historical/biographical PDFs
   - Processed as `pdf_with_century` type

2. **Regulations** (`Resources/PDFs/`)
   - North Korean regulations and documents
   - Processed as `pdf_regulations` type

### 3. PDF Text Cleaning
Automatic cleaning includes:
- ‚úÖ Remove page numbers (standalone digits)
- ‚úÖ Remove headers/footers (low Korean content)
- ‚úÖ Remove short artifacts (< 5 characters)
- ‚úÖ Validate minimum Korean content (50%+ Korean characters)
- ‚úÖ Skip documents with < 100 characters after cleaning

### 4. Smart Text Chunking
- Long PDFs split into ~2000 character chunks
- Splits on sentence boundaries (. ! ?)
- Each chunk tracked with `chunk_index`
- Prevents exceeding BERT's context window

### 5. Checkpointing Support
- PDF processing results cached
- Fast resume on interruption
- Delete `data_processing_checkpoints/pdfs.json` to reprocess

## Code Added

### Main Functions
```python
extract_pdf_text(pdf_path: Path) -> str
    - Extract text using PyMuPDF with PyPDF2 fallback

clean_pdf_text(text: str) -> str
    - Remove page numbers, headers, footers
    - Validate Korean content ratio

process_pdfs() -> List[Dict[str, Any]]
    - Process both PDF folders
    - Clean and chunk text
    - Save checkpoints
```

## Testing Results

‚úÖ **Syntax Check**: Passes  
‚úÖ **Import Check**: Passes  
‚úÖ **Runtime Check**: Executes successfully  
‚úÖ **PDF Libraries**: Installed and detected  

## Usage

### Install PDF Libraries (if needed)
```bash
pip install PyPDF2 PyMuPDF
```

### Run Data Processing
```bash
python3 train_with_local_data.py
```

The script will:
1. Load existing checkpoints for speeches, parallel, dictionaries, gyeoremal
2. Process PDFs from both folders (or load from checkpoint)
3. Create combined training dataset
4. Output to `local_training_data/train.json` and `validation.json`

### Output Example
```
Sources used:
  ‚úÖ Kim's New Year Speeches
  ‚úÖ Parallel translation pairs (NK/SK)
  ‚úÖ NK dictionaries (10,000 entries)
  ‚úÖ Gyeoremal dialect comparisons
  ‚úÖ PDFs (With The Century, regulations)
```

## Data Quality Controls

1. **Korean Content Validation**
   - Minimum 50% Korean characters
   - Minimum 100 characters after cleaning

2. **Artifact Removal**
   - Page numbers filtered
   - Headers/footers removed
   - Short lines excluded

3. **Chunk Size Management**
   - Maximum 2000 characters per chunk
   - Sentence-aware splitting
   - Preserves context

## Performance

- **Checkpointing**: Processed PDFs cached for instant reuse
- **Dual Extraction**: PyMuPDF primary, PyPDF2 fallback ensures coverage
- **Memory Efficient**: Processes one PDF at a time

## Statistics in Output

After processing, you'll see breakdown like:
```
üìä Data sources breakdown:
   kim: 7 items
   parallel: 1924 items
   dictionary: 10000 items
   gyeoremal: 248 items
   pdf: 156 items  ‚Üê New!
```

## Troubleshooting

### If PDF libraries not installed:
```
Warning: PDF libraries not available. Install with: pip install PyPDF2 PyMuPDF
Sources used:
  ...
  ‚ö†Ô∏è  PDFs (libraries not installed)
```

**Fix**: `pip install PyPDF2 PyMuPDF`

### If no PDFs found:
```
[INFO] Warning: /path/to/Resources/PDFs not found
[INFO] Found 0 text chunks from PDFs
```

**Fix**: Ensure PDF folders exist with .pdf files

### If text extraction fails:
- Check PDF is not password-protected
- Check PDF contains actual text (not just images)
- Review logs for specific error messages

## Integration with Training Pipeline

The processed PDF data integrates seamlessly:

1. **Data Format**: Same as other sources
   ```python
   {
     "text": "cleaned Korean text...",
     "source": "pdf_with_century_filename",
     "type": "pdf_with_century",
     "chunk_index": 0  # if chunked
   }
   ```

2. **Training**: Use with DPRK-BERT mlm_trainer.py
   ```bash
   cd DPRK-BERT-master
   python mlm_trainer.py --mode train \
     --train_file ../local_training_data/train.json \
     --validation_file ../local_training_data/validation.json
   ```

3. **90/10 Split**: PDFs included in train/validation split automatically

## Next Steps

1. **Run processing**: `python3 train_with_local_data.py`
2. **Check output**: Review `local_training_data/` for generated files
3. **Inspect samples**: Look at PDF chunks in the JSON output
4. **Train model**: Use generated dataset with mlm_trainer.py

---

**Status**: ‚úÖ PDF processing fully integrated and tested  
**Dependencies**: PyPDF2, PyMuPDF (both installed)  
**Compatibility**: Works with existing checkpoint system
